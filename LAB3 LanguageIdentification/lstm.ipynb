{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\nimport random\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch # Deep learning framework\nimport torch.nn.functional as F\nimport time\n\n# Input data files are available in the \"../input/\" directory.\nimport os\nINPUTDIR = '../input/wili6'\nprint(os.listdir(f'{INPUTDIR}'))\n\n#Init random seed to get reproducible results\nseed = 1111\nrandom.seed(seed)\nnp.random.RandomState(seed)\ntorch.manual_seed(seed)\n\n# Any results you write to the current directory are saved as output.\nx_train_full = open(f'{INPUTDIR}/x_train.txt').read().splitlines()\ny_train_full = open(f'{INPUTDIR}/y_train.txt').read().splitlines()\nprint('Example:')\nprint('LANG =', y_train_full[0])\nprint('TEXT =', x_train_full[0])","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2023-04-27T08:47:04.428352Z","iopub.execute_input":"2023-04-27T08:47:04.428656Z","iopub.status.idle":"2023-04-27T08:47:09.311430Z","shell.execute_reply.started":"2023-04-27T08:47:04.428625Z","shell.execute_reply":"2023-04-27T08:47:09.305131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Dictionary(object):\n    def __init__(self):\n        self.token2idx = {}\n        self.idx2token = []\n\n    def add_token(self, token):\n        if token not in self.token2idx:\n            self.idx2token.append(token)\n            self.token2idx[token] = len(self.idx2token) - 1\n        return self.token2idx[token]\n\n    def __len__(self):\n        return len(self.idx2token)","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2023-04-27T08:47:09.320068Z","iopub.execute_input":"2023-04-27T08:47:09.321585Z","iopub.status.idle":"2023-04-27T08:47:09.340496Z","shell.execute_reply.started":"2023-04-27T08:47:09.321534Z","shell.execute_reply":"2023-04-27T08:47:09.336137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The **Dictionary** class is used to map tokens (characters, words, subwords) into consecutive integer indexes.  \nThe index **0** is reserved for padding sequences up to a fixed lenght, and the index **1** for any 'unknown' character","metadata":{}},{"cell_type":"code","source":"char_vocab = Dictionary()\npad_token = '<pad>' # reserve index 0 for padding\nunk_token = '<unk>' # reserve index 1 for unknown token\npad_index = char_vocab.add_token(pad_token)\nunk_index = char_vocab.add_token(unk_token)\n\n# join all the training sentences in a single string\n# and obtain the list of different characters with set\nchars = set(''.join(x_train_full))\nfor char in sorted(chars):\n    char_vocab.add_token(char)\nprint(\"Vocabulary:\", len(char_vocab), \"UTF characters\")\n\nlang_vocab = Dictionary()\n# use python set to obtain the list of languages without repetitions\nlanguages = set(y_train_full)\nfor lang in sorted(languages):\n    lang_vocab.add_token(lang)\nprint(\"Labels:\", len(lang_vocab), \"languages\")","metadata":{"execution":{"iopub.status.busy":"2023-04-13T20:54:21.614377Z","iopub.execute_input":"2023-04-13T20:54:21.618324Z","iopub.status.idle":"2023-04-13T20:54:23.503406Z","shell.execute_reply.started":"2023-04-13T20:54:21.618281Z","shell.execute_reply":"2023-04-13T20:54:23.502256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#From token or label to index\nprint('a ->', char_vocab.token2idx['a'])\nprint('cat ->', lang_vocab.token2idx['cat'])\nprint(y_train_full[0], x_train_full[0][:10])\nx_train_idx = [np.array([char_vocab.token2idx[c] for c in line]) for line in x_train_full]\ny_train_idx = np.array([lang_vocab.token2idx[lang] for lang in y_train_full])\nprint(y_train_idx[0], x_train_idx[0][:10])","metadata":{"execution":{"iopub.status.busy":"2023-04-13T20:54:23.506146Z","iopub.execute_input":"2023-04-13T20:54:23.507015Z","iopub.status.idle":"2023-04-13T20:54:32.142664Z","shell.execute_reply.started":"2023-04-13T20:54:23.506973Z","shell.execute_reply":"2023-04-13T20:54:32.141536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Radomly select 15% of the database for validation  \nCreate lists of (input, target) tuples for training and validation","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(x_train_idx, y_train_idx, test_size=0.15, random_state=seed)\ntrain_data = [(x, y) for x, y in zip(x_train, y_train)]\nval_data = [(x, y) for x, y in zip(x_val, y_val)]\nprint(len(train_data), \"training samples\")\nprint(len(val_data), \"validation samples\")","metadata":{"execution":{"iopub.status.busy":"2023-04-13T20:54:32.144269Z","iopub.execute_input":"2023-04-13T20:54:32.144930Z","iopub.status.idle":"2023-04-13T20:54:32.772051Z","shell.execute_reply.started":"2023-04-13T20:54:32.144887Z","shell.execute_reply":"2023-04-13T20:54:32.770837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def batch_generator(data, batch_size, token_size):\n    \"\"\"Yield elements from data in chunks with a maximum of batch_size sequences and token_size tokens.\"\"\"\n    minibatch, sequences_so_far, tokens_so_far = [], 0, 0\n    for ex in data:\n        seq_len = len(ex[0])\n        if seq_len > token_size:\n            ex = (ex[0][:token_size], ex[1])\n            seq_len = token_size\n        minibatch.append(ex)\n        sequences_so_far += 1\n        tokens_so_far += seq_len\n        if sequences_so_far == batch_size or tokens_so_far == token_size:\n            yield minibatch\n            minibatch, sequences_so_far, tokens_so_far = [], 0, 0\n        elif sequences_so_far > batch_size or tokens_so_far > token_size:\n            yield minibatch[:-1]\n            minibatch, sequences_so_far, tokens_so_far = minibatch[-1:], 1, len(minibatch[-1][0])\n    if minibatch:\n        yield minibatch","metadata":{"execution":{"iopub.status.busy":"2023-04-13T20:54:32.773942Z","iopub.execute_input":"2023-04-13T20:54:32.774643Z","iopub.status.idle":"2023-04-13T20:54:32.785828Z","shell.execute_reply.started":"2023-04-13T20:54:32.774603Z","shell.execute_reply":"2023-04-13T20:54:32.784560Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pool_generator(data, batch_size, token_size, shuffle=False):\n    \"\"\"Sort within buckets, then batch, then shuffle batches.\n    Partitions data into chunks of size 100*token_size, sorts examples within\n    each chunk, then batch these examples and shuffle the batches.\n    \"\"\"\n    for p in batch_generator(data, batch_size * 100, token_size * 100):\n        p_batch = batch_generator(sorted(p, key=lambda t: len(t[0]), reverse=True), batch_size, token_size)\n        p_list = list(p_batch)\n        if shuffle:\n            for b in random.sample(p_list, len(p_list)):\n                yield b\n        else:\n            for b in p_list:\n                yield b","metadata":{"execution":{"iopub.status.busy":"2023-04-13T20:54:32.787649Z","iopub.execute_input":"2023-04-13T20:54:32.788655Z","iopub.status.idle":"2023-04-13T20:54:32.801471Z","shell.execute_reply.started":"2023-04-13T20:54:32.788588Z","shell.execute_reply":"2023-04-13T20:54:32.797663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**DNN Model**  \nIncludes Python comments with the dimension of the input  matrix:  \nT = Max number of tokens in a sequence  \nB = Number of sequences (batch size)  \nE = Embedding dim  \nH = Hidden size  \nO = Output size (number of languages)","metadata":{}},{"cell_type":"code","source":"x = torch.Tensor(2,4,2)\ny = torch.Tensor(2,4,2)\nxm,_ = x.max(dim=0)\nym = y.mean(dim=0)\nsuma = torch.add(xm, ym)\nsuma.size()","metadata":{"execution":{"iopub.status.busy":"2023-04-13T20:54:32.803812Z","iopub.execute_input":"2023-04-13T20:54:32.804263Z","iopub.status.idle":"2023-04-13T20:54:32.883320Z","shell.execute_reply.started":"2023-04-13T20:54:32.804193Z","shell.execute_reply":"2023-04-13T20:54:32.882407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CharRNNClassifier(torch.nn.Module):\n\n    def __init__(self, input_size, embedding_size, hidden_size, output_size, model=\"lstm\", num_layers=3, bidirectional=True, pad_idx=0):\n        super().__init__()\n        self.model = model.lower()\n        self.hidden_size = hidden_size\n        self.embed = torch.nn.Embedding(input_size, embedding_size, padding_idx=pad_idx)\n        if self.model == \"gru\":\n            self.rnn = torch.nn.GRU(embedding_size, hidden_size, num_layers, bidirectional=bidirectional)\n        elif self.model == \"lstm\":\n            self.rnn = torch.nn.LSTM(embedding_size, hidden_size, num_layers, bidirectional=bidirectional)\n        self.h2o = torch.nn.Linear(2*hidden_size, output_size)\n        self.dropout = torch.nn.Dropout(p = 0.4)\n        \n    def forward(self, input, input_lengths):\n        # T x B\n        encoded = self.embed(input)\n        # T x B x E\n        packed = torch.nn.utils.rnn.pack_padded_sequence(encoded, input_lengths)\n        # Packed T x B x E\n        output, _ = self.rnn(packed)\n        # Packed T x B x H\n        # Important: you may need to replace '-inf' with the default zero padding for other pooling layers\n        padded_max, _ = torch.nn.utils.rnn.pad_packed_sequence(output, padding_value=float('-inf'))\n        padded_avg, _ = torch.nn.utils.rnn.pad_packed_sequence(output, padding_value=float(0))\n        # T x B x H\n        output1, _ = padded_max.max(dim=0)\n        output2 = padded_avg.mean(dim=0)\n        output = torch.add(output1,output2)\n        output = self.dropout(output)\n        # B x 2H\n        output = self.h2o(output)\n        # B x O\n        return output","metadata":{"execution":{"iopub.status.busy":"2023-04-13T20:56:27.607603Z","iopub.execute_input":"2023-04-13T20:56:27.608214Z","iopub.status.idle":"2023-04-13T20:56:27.620533Z","shell.execute_reply.started":"2023-04-13T20:56:27.608177Z","shell.execute_reply":"2023-04-13T20:56:27.619518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not torch.cuda.is_available():\n    print(\"WARNING: CUDA is not available. Select 'GPU On' on kernel settings\")\ndevice = torch.device(\"cuda\")\ntorch.cuda.manual_seed(seed)","metadata":{"execution":{"iopub.status.busy":"2023-04-13T20:56:28.183489Z","iopub.execute_input":"2023-04-13T20:56:28.183857Z","iopub.status.idle":"2023-04-13T20:56:28.189685Z","shell.execute_reply.started":"2023-04-13T20:56:28.183824Z","shell.execute_reply":"2023-04-13T20:56:28.188455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The **nn.CrossEntropyLoss()** criterion combines **nn.LogSoftmax()** and **nn.NLLLoss()** in one single class.  \nIt is useful when training a classification problem.","metadata":{}},{"cell_type":"code","source":"criterion = torch.nn.CrossEntropyLoss(reduction='sum')","metadata":{"execution":{"iopub.status.busy":"2023-04-13T20:56:29.008703Z","iopub.execute_input":"2023-04-13T20:56:29.009082Z","iopub.status.idle":"2023-04-13T20:56:29.014864Z","shell.execute_reply.started":"2023-04-13T20:56:29.009040Z","shell.execute_reply":"2023-04-13T20:56:29.013632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, optimizer, data, batch_size, token_size, max_norm=1, log=False):\n    model.train()\n    total_loss = 0\n    ncorrect = 0\n    nsentences = 0\n    ntokens = 0\n    niterations = 0\n    for batch in pool_generator(data, batch_size, token_size, shuffle=True):\n        # Get input and target sequences from batch\n        X = [torch.from_numpy(d[0]) for d in batch]\n        X_lengths = [x.numel() for x in X]\n        ntokens += sum(X_lengths)\n        X_lengths = torch.tensor(X_lengths, dtype=torch.long)\n        y = torch.tensor([d[1] for d in batch], dtype=torch.long, device=device)\n        # Pad the input sequences to create a matrix\n        X = torch.nn.utils.rnn.pad_sequence(X).to(device)\n        model.zero_grad()\n        output = model(X, X_lengths)\n        loss = criterion(output, y)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)      # Gradient clipping https://www.kaggle.com/c/wili4/discussion/231378\n        optimizer.step()\n        # Training statistics\n        total_loss += loss.item()\n        ncorrect += (torch.max(output, 1)[1] == y).sum().item()\n        nsentences += y.numel()\n        niterations += 1\n    \n    total_loss = total_loss / nsentences\n    accuracy = 100 * ncorrect / nsentences\n    if log:\n        print(f'Train: wpb={ntokens//niterations}, bsz={nsentences//niterations}, num_updates={niterations}')\n    return accuracy","metadata":{"execution":{"iopub.status.busy":"2023-04-13T20:56:29.263627Z","iopub.execute_input":"2023-04-13T20:56:29.264517Z","iopub.status.idle":"2023-04-13T20:56:29.277603Z","shell.execute_reply.started":"2023-04-13T20:56:29.264475Z","shell.execute_reply":"2023-04-13T20:56:29.275919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validate(model, data, batch_size, token_size):\n    model.eval()\n    # calculate accuracy on validation set\n    ncorrect = 0\n    nsentences = 0\n    with torch.no_grad():\n        for batch in pool_generator(data, batch_size, token_size):\n            # Get input and target sequences from batch\n            X = [torch.from_numpy(d[0]) for d in batch]\n            X_lengths = torch.tensor([x.numel() for x in X], dtype=torch.long)\n            y = torch.tensor([d[1] for d in batch], dtype=torch.long, device=device)\n            # Pad the input sequences to create a matrix\n            X = torch.nn.utils.rnn.pad_sequence(X).to(device)\n            answer = model(X, X_lengths)\n            ncorrect += (torch.max(answer, 1)[1] == y).sum().item()\n            nsentences += y.numel()\n        dev_acc = 100 * ncorrect / nsentences\n    return dev_acc","metadata":{"execution":{"iopub.status.busy":"2023-04-13T20:56:29.635995Z","iopub.execute_input":"2023-04-13T20:56:29.638329Z","iopub.status.idle":"2023-04-13T20:56:29.651092Z","shell.execute_reply.started":"2023-04-13T20:56:29.638281Z","shell.execute_reply":"2023-04-13T20:56:29.649986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hidden_size = 256\nembedding_size = 64\nbidirectional = True\nntokens = len(char_vocab)\nnlabels = len(lang_vocab)","metadata":{"execution":{"iopub.status.busy":"2023-04-13T20:56:30.733627Z","iopub.execute_input":"2023-04-13T20:56:30.733992Z","iopub.status.idle":"2023-04-13T20:56:30.739406Z","shell.execute_reply.started":"2023-04-13T20:56:30.733959Z","shell.execute_reply":"2023-04-13T20:56:30.738270Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model for cross-validation","metadata":{}},{"cell_type":"code","source":"def get_model():\n    model = CharRNNClassifier(ntokens, embedding_size, hidden_size, nlabels, bidirectional=bidirectional, pad_idx=pad_index).to(device)\n    optimizer = torch.optim.Adam(model.parameters())\n    return model, optimizer","metadata":{"execution":{"iopub.status.busy":"2023-04-13T20:56:32.184745Z","iopub.execute_input":"2023-04-13T20:56:32.185123Z","iopub.status.idle":"2023-04-13T20:56:32.190661Z","shell.execute_reply.started":"2023-04-13T20:56:32.185088Z","shell.execute_reply":"2023-04-13T20:56:32.189597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size, token_size = 256, 200000\nepochs = 25\ntrain_accuracy = []\nvalid_accuracy = []\nmodel, optimizer = get_model()\nprint(f'Training cross-validation model for {epochs} epochs')\nt0 = time.time()\nfor epoch in range(1, epochs + 1):\n    acc = train(model, optimizer, train_data, batch_size, token_size, log=epoch==1)\n    train_accuracy.append(acc)\n    print(f'| epoch {epoch:03d} | train accuracy={acc:.1f}% ({time.time() - t0:.0f}s)')\n    acc = validate(model, val_data, batch_size, token_size)\n    valid_accuracy.append(acc)\n    print(f'| epoch {epoch:03d} | valid accuracy={acc:.1f}%')","metadata":{"execution":{"iopub.status.busy":"2023-04-13T20:56:32.727802Z","iopub.execute_input":"2023-04-13T20:56:32.728165Z","iopub.status.idle":"2023-04-13T20:56:54.350948Z","shell.execute_reply.started":"2023-04-13T20:56:32.728132Z","shell.execute_reply":"2023-04-13T20:56:54.349090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.memory_summary(device=None, abbreviated=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-13T20:56:54.352559Z","iopub.status.idle":"2023-04-13T20:56:54.353574Z","shell.execute_reply.started":"2023-04-13T20:56:54.353266Z","shell.execute_reply":"2023-04-13T20:56:54.353296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model)\nfor name, param in model.named_parameters():\n    print(f'{name:20} {param.numel()} {list(param.shape)}')\nprint(f'TOTAL                {sum(p.numel() for p in model.parameters())}')","metadata":{"execution":{"iopub.status.busy":"2023-04-13T20:55:25.497990Z","iopub.status.idle":"2023-04-13T20:55:25.498520Z","shell.execute_reply.started":"2023-04-13T20:55:25.498276Z","shell.execute_reply":"2023-04-13T20:55:25.498302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(range(1, len(train_accuracy)+1), train_accuracy)\nplt.plot(range(1, len(valid_accuracy)+1), valid_accuracy)\nplt.xlabel('epoch')\nplt.ylabel('Accuracy');","metadata":{"execution":{"iopub.status.busy":"2023-04-13T20:55:25.500369Z","iopub.status.idle":"2023-04-13T20:55:25.500868Z","shell.execute_reply.started":"2023-04-13T20:55:25.500605Z","shell.execute_reply":"2023-04-13T20:55:25.500631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Final model**  \nFinally, we create a model using all the training data and we generate the submission with the predicted test labels","metadata":{}},{"cell_type":"code","source":"print(f'Training final model for {epochs} epochs')\nmodel, optimizer = get_model()\nt0 = time.time()\nfor epoch in range(1, epochs + 1):\n    acc = train(model, optimizer, train_data + val_data, batch_size, token_size, log=epoch==1)\n    print(f'| epoch {epoch:03d} | train accuracy={acc:.3f} ({time.time() - t0:.0f}s)')","metadata":{"execution":{"iopub.status.busy":"2023-04-13T20:55:25.502975Z","iopub.status.idle":"2023-04-13T20:55:25.503479Z","shell.execute_reply.started":"2023-04-13T20:55:25.503224Z","shell.execute_reply":"2023-04-13T20:55:25.503250Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test(model, data, batch_size, token_size):\n    model.eval()\n    sindex = []\n    labels = []\n    with torch.no_grad():\n        for batch in pool_generator(data, batch_size, token_size):\n            # Get input sequences from batch\n            X = [torch.from_numpy(d[0]) for d in batch]\n            X_lengths = torch.tensor([x.numel() for x in X], dtype=torch.long)\n            # Pad the input sequences to create a matrix\n            X = torch.nn.utils.rnn.pad_sequence(X).to(device)\n            answer = model(X, X_lengths)\n            label = torch.max(answer, 1)[1].cpu().numpy()\n            # Save labels and sentences index\n            labels.append(label)\n            sindex += [d[1] for d in batch]\n    return np.array(sindex), np.concatenate(labels)","metadata":{"execution":{"iopub.status.busy":"2023-04-13T20:55:25.505316Z","iopub.status.idle":"2023-04-13T20:55:25.505802Z","shell.execute_reply.started":"2023-04-13T20:55:25.505546Z","shell.execute_reply":"2023-04-13T20:55:25.505570Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the test database we replace the label (language) with a sentence index.  ","metadata":{}},{"cell_type":"code","source":"x_test_txt = open(f'{INPUTDIR}/x_test.txt').read().splitlines()\nx_test_idx = [np.array([char_vocab.token2idx[c] if c in char_vocab.token2idx else unk_index for c in line]) for line in x_test_txt]\ntest_data = [(x, idx) for idx, x in enumerate(x_test_idx)]","metadata":{"execution":{"iopub.status.busy":"2023-04-10T11:34:04.086160Z","iopub.status.idle":"2023-04-10T11:34:04.087253Z","shell.execute_reply.started":"2023-04-10T11:34:04.086877Z","shell.execute_reply":"2023-04-10T11:34:04.086925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The sentence index is used to rearrange the labels in the original sentence order","metadata":{}},{"cell_type":"code","source":"index, labels = test(model, test_data, batch_size, token_size)\norder = np.argsort(index)\nlabels = labels[order]","metadata":{"execution":{"iopub.status.busy":"2023-04-10T11:34:04.088804Z","iopub.status.idle":"2023-04-10T11:34:04.089747Z","shell.execute_reply.started":"2023-04-10T11:34:04.089465Z","shell.execute_reply":"2023-04-10T11:34:04.089494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('submission.csv', 'w') as f:\n    print('Id,Language', file=f)\n    for sentence_id, lang_id in enumerate(labels):\n        language = lang_vocab.idx2token[lang_id]\n        if sentence_id < 10:\n            print(f'{sentence_id},{language}')\n        print(f'{sentence_id},{language}', file=f)","metadata":{"execution":{"iopub.status.busy":"2023-04-10T11:34:04.091554Z","iopub.status.idle":"2023-04-10T11:34:04.092483Z","shell.execute_reply.started":"2023-04-10T11:34:04.092185Z","shell.execute_reply":"2023-04-10T11:34:04.092213Z"},"trusted":true},"execution_count":null,"outputs":[]}]}